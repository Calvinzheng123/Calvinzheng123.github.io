<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Project 2: Fraud Detection — Bank Account Fraud (BAF Base)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <script src="https://cdn.tailwindcss.com"></script>
  <meta name="description" content="Fraud detection classification with Logistic Regression and Random Forest on the Feedzai BAF Base dataset. Includes EDA visuals, evaluation, threshold tuning, impact."/>
</head>
<body class="bg-white text-gray-900">
  <header class="max-w-3xl mx-auto p-6">
    <a href="index.html" class="text-sm text-blue-600 underline">← Back</a>
    <h1 class="text-3xl font-bold mt-2">Project 2: Fraud Detection — Bank Account Fraud (BAF Base)</h1>
    <p class="mt-2 text-gray-700">
      Dataset:
      <a class="underline text-blue-600" href="https://github.com/feedzai/bank-account-fraud" target="_blank" rel="noopener">Feedzai — Bank Account Fraud (BAF)</a>
      (<a class="underline text-blue-600" href="https://www.kaggle.com/datasets/whenamancodes/bank-account-fraud-baf-dataset" target="_blank" rel="noopener">Kaggle mirror</a>).
    </p>
    <p class="mt-1 text-gray-700">Goal: binary classification — predict <code>fraud_bool</code> (1 = fraud, 0 = non-fraud).</p>
  </header>

  <main class="max-w-3xl mx-auto p-6 space-y-10">
    <!-- ===== Storytelling / Portfolio Post ===== -->
    <section>
      <h2 class="text-2xl font-semibold">Story: Detecting fraud under heavy imbalance</h2>
      <p class="mt-2">
        Fraud is rare in the real world, and it was just as rare in this dataset (about 1–2% of all rows). 
        I compared two classifiers, Logistic Regression as a baseline and Random Forest as a nonlinear model 
        to see how well they could separate fraud from non-fraud. I also experimented with adjusting decision thresholds, 
        since catching fraud often means trading precision for higher recall.
      </p>

      <!-- Problem -->
      <div class="mt-6">
        <h3 class="text-xl font-semibold">Problem</h3>
        <p class="mt-1">
          The main question was whether we could reliably classify transactions as fraud vs. non-fraud. 
          From there, I wanted to see what trade-offs exist between recall (catching fraud) and precision (avoiding false alarms), 
          and which features mattered most for driving fraud predictions.
        </p>
      </div>

      <!-- Data -->
      <div class="mt-6">
        <h3 class="text-xl font-semibold">Data</h3>
        <p class="mt-1">
          The dataset had about 1.1M rows with the target <code>fraud_bool</code>. 
          Features included demographics (age and income buckets, employment), 
          geography (zip codes), transaction summaries, and a <code>month</code> field. 
          Several columns were anonymized or bucketed. Because the fraud rate was so low, 
          evaluation features had to be chosen carefully and accuracy alone wasn’t enough.
        </p>
      </div>

      <!-- Visuals -->
      <div class="mt-6">
        <h3 class="text-xl font-semibold">Visualizations</h3>
        <div class="space-y-6 mt-2">
          <figure>
            <img src="figs/baf_class_balance.png" alt="Fraud vs non-fraud class counts bar chart" class="w-full rounded border" />
            <figcaption class="text-sm text-gray-600 mt-1">Class imbalance was severe: fraud made up only ~1–2% of rows.</figcaption>
          </figure>
          <figure>
            <img src="figs/baf_corr_heatmap.png" alt="Correlation heatmap of numeric features and target" class="w-full rounded border" />
            <figcaption class="text-sm text-gray-600 mt-1">Numeric correlations with fraud were weak, suggesting nonlinear models might be more effective.</figcaption>
          </figure>
          <figure>
            <img src="figs/baf_roc_compare.png" alt="ROC curves for Logistic Regression and Random Forest" class="w-full rounded border" />
            <figcaption class="text-sm text-gray-600 mt-1">ROC curves showed Random Forest separated the classes more effectively than Logistic Regression.</figcaption>
          </figure>
          <figure>
            <img src="figs/baf_pr_compare.png" alt="Precision-Recall curves for Logistic Regression and Random Forest" class="w-full rounded border" />
            <figcaption class="text-sm text-gray-600 mt-1">PR curves were more informative under imbalance; Random Forest had a stronger PR-AUC.</figcaption>
          </figure>
          <figure>
            <img src="figs/baf_rf_importance.png" alt="Top feature importances from Random Forest" class="w-full rounded border" />
            <figcaption class="text-sm text-gray-600 mt-1">Top features (like address stability, credit risk, and device OS) made intuitive sense as fraud indicators.</figcaption>
          </figure>
        </div>
      </div>

      <!-- Results & Storytelling -->
      <div class="mt-6">
        <h3 class="text-xl font-semibold">Results & Storytelling</h3>
        <p class="mt-2">
          Logistic Regression struggled, with a PR-AUC of only 0.046. Random Forest performed much better with a PR-AUC of 0.13 and ROC-AUC of 0.876. 
          Lowering the Random Forest threshold from the default 0.5 to about 0.31 boosted recall above 85%, meaning the majority of frauds were caught. 
          The trade-off was precision dropping to around 3–4%. In the context of fraud detection, this seemed like a reasonable balance, 
          since it is usually worse to miss fraud than to review a few extra false alarms.
        </p>
        <p class="mt-2">
          The confusion matrix at this threshold showed around 1,890 frauds caught, 316 missed, but also around 54,000 false positives. 
          Even with a good model, the imbalance creates challenges. Still, the recall gain demonstrated the value of tuning thresholds.  
          Feature importance results added interpretability: unstable addresses, Windows devices, and weak credit scores stood out, 
          all of which match what you might expect from real fraud risk factors.
        </p>
      </div>

      <!-- Learnings -->
      <div class="mt-6">
        <h3 class="text-xl font-semibold">Key Takeaways</h3>
        <ul class="list-disc pl-6 mt-2 space-y-1">
          <li>Accuracy looked high (90%) but was misleading; PR-AUC and recall were the real indicators of performance.</li>
          <li>Logistic Regression worked as a baseline, but Random Forest provided much stronger separation and more useful trade-offs.</li>
          <li>Threshold choice was just as important as the model — small changes had a big effect on recall and precision.</li>
          <li>Feature importances connected model behavior to real-world fraud patterns, which made the results easier to interpret.</li>
        </ul>
      </div>

      <!-- Impact & Limits -->
      <div class="mt-6">
        <h3 class="text-xl font-semibold">Impact & Limits</h3>
        <p class="mt-1">
          Detecting more fraud has clear benefits in reducing financial losses, but the large number of false positives can create 
          friction for legitimate customers. There are also ethical considerations: some features (like zip codes or income) could 
          act as proxies for demographics, so it’s important to check for fairness across subgroups. The dataset itself was anonymized 
          and static, so results don’t necessarily capture how fraud evolves over time, but the exercise highlighted the challenges and 
          trade-offs of real-world fraud detection.
        </p>
      </div>
    </section>

    <!-- ===== Technical Report ===== -->
    <section class="border-t pt-8">
      <h2 class="text-2xl font-semibold">Technical Report (Process & Reflection)</h2>

      <nav class="mt-4 text-sm text-blue-700">
        <ul class="list-disc pl-6 space-y-1">
          <li><a href="#dataset-origin" class="underline">Dataset origin & description</a></li>
          <li><a href="#preprocessing" class="underline">Preprocessing</a></li>
          <li><a href="#viz" class="underline">Data understanding & visuals</a></li>
          <li><a href="#modeling" class="underline">Modeling</a></li>
          <li><a href="#evaluation" class="underline">Evaluation & thresholds</a></li>
          <li><a href="#impact" class="underline">Impact</a></li>
          <li><a href="#code" class="underline">Code & notebook</a></li>
          <li><a href="#refs" class="underline">References</a></li>
        </ul>
      </nav>

      <!-- Dataset origin -->
      <section id="dataset-origin" class="mt-6">
        <h3 class="text-xl font-semibold">Dataset origin & description</h3>
        <p class="mt-2">
          Source: <a class="underline text-blue-600" href="https://github.com/feedzai/bank-account-fraud" target="_blank" rel="noopener">Feedzai BAF</a> 
          (with a <a class="underline text-blue-600" href="https://www.kaggle.com/datasets/whenamancodes/bank-account-fraud-baf-dataset" target="_blank" rel="noopener">Kaggle mirror</a>). 
          I used the <strong>Base</strong> variant, where the target is <code>fraud_bool</code>. 
          Features spanned demographics, geography, transaction summaries, and month. The fraud rate was only about 1–2%.
        </p>
      </section>

      <!-- Preprocessing -->
      <section id="preprocessing" class="mt-6">
        <h3 class="text-xl font-semibold">Preprocessing (with rationale)</h3>
        <ul class="list-disc pl-6 mt-2 space-y-2">
          <li>Converted <code>fraud_bool</code> to an integer target (0/1).</li>
          <li>Expanded categorical features into dummies with <code>pd.get_dummies(drop_first=True)</code> for model compatibility.</li>
          <li>Used an 80/20 stratified train/test split to preserve the fraud ratio.</li>
          <li>Applied <code>class_weight="balanced"</code> in both models to help offset class imbalance.</li>
          <li>Considered bucketing rare categories as <code>__OTHER__</code> to reduce feature explosion for speed.</li>
        </ul>
      </section>

      <!-- Viz -->
      <section id="viz" class="mt-6">
        <h3 class="text-xl font-semibold">Data understanding & visuals</h3>
        <p class="mt-2">
          Early exploration confirmed the severe class imbalance and correlation heatmaps 
          showed that linear models would have limited success, which supported the decision to try Random Forest for nonlinear interactions.
        </p>
      </section>

      <!-- Modeling -->
      <section id="modeling" class="mt-6">
        <h3 class="text-xl font-semibold">Modeling</h3>
        <ul class="list-disc pl-6 mt-2 space-y-2">
          <li><strong>Logistic Regression</strong> — a linear baseline. It was slow to train and while interpretable, it struggled with convergence and gave very low precision.</li>
          <li><strong>Random Forest</strong> — an ensemble of decision trees. It handled nonlinear relationships and interactions better and delivered stronger ROC-AUC and PR-AUC scores.</li>
        </ul>
      </section>

      <!-- Evaluation -->
      <section id="evaluation" class="mt-6">
        <h3 class="text-xl font-semibold">Evaluation & thresholds</h3>
        <ul class="list-disc pl-6 mt-2 space-y-2">
          <li><strong>Metrics:</strong> ROC-AUC and PR-AUC were most important for me, Precision, Recall, and F1 provided more operational context but were standard so I still found out what they were.</li>
          <li><strong>Example results:</strong>
            <ul class="list-disc pl-6 mt-1">
              <li>Logistic Regression — ROC-AUC = 0.66, PR-AUC = 0.02, Recall = 0.52, Precision = 0.02.</li>
              <li>Random Forest — ROC-AUC = 0.876, PR-AUC = 0.13, Recall = 0.63, Precision = 0.07.</li>
            </ul>
          </li>
          <li>Lowering the Random Forest threshold boosted recall above 80% but dropped precision to single digits which a trade-off often seen in fraud detection.</li>
        </ul>
      </section>

      <!-- Impact -->
      <section id="impact" class="mt-6">
        <h3 class="text-xl font-semibold">Impact</h3>
        <p class="mt-2">
          Capturing more fraud reduces direct financial losses, but too many false positives could frustrate customers. 
          Ethical considerations include the possibility of demographic bias if features like income or zip code serve as indirect proxies. 
          Since the dataset was static and anonymized, the results don’t capture evolving fraud strategies, but they highlight 
          the real challenges of balancing fraud detection performance with fairness and customer experience.
        </p>
      </section>

      <!-- Code & Notebook links -->
      <section id="code" class="mt-6">
        <h3 class="text-xl font-semibold">Code & Notebook</h3>
        <p class="mt-1">Reproducible assets:</p>
        <div class="space-y-2">
          <p><a class="underline text-blue-600" href="notebooks/Fraud.ipynb" download>Download the Jupyter Notebook</a></p>
          <div class="space-x-3 mt-2">
            <a class="inline-block px-3 py-2 rounded bg-black text-white" href="https://colab.research.google.com/github/Calvinzheng123/Calvinzheng123.github.io/blob/main/notebooks/Fraud.ipynb" target="_blank" rel="noopener">Open in Colab</a>
            <a class="inline-block px-3 py-2 rounded bg-gray-200" href="https://nbviewer.org/github/Calvinzheng123/Calvinzheng123.github.io/blob/main/notebooks/Fraud.ipynb" target="_blank" rel="noopener">View on nbviewer</a>
          </div>
        </div>
      </section>

      <!-- References -->
      <section id="refs" class="mt-8">
        <h3 class="text-xl font-semibold">References</h3>
        <ul class="list-disc pl-6 mt-2">
          <li>Dataset: <a class="underline text-blue-600" href="https://github.com/feedzai/bank-account-fraud" target="_blank" rel="noopener">Feedzai BAF (Base)</a> / <a class="underline text-blue-600" href="https://www.kaggle.com/datasets/whenamancodes/bank-account-fraud-baf-dataset" target="_blank" rel="noopener">Kaggle mirror</a></li>
          <li>Libraries: pandas, scikit-learn, matplotlib, seaborn</li>
          <li>Transparency: ChatGPT helped organize the page structure and polish the write-up as well as help with brainstorming ideas such as the threshold and class imbalance solutions as well as implementing them.</li>
        </ul>
      </section>
    </section>
  </main>

  <footer class="max-w-3xl mx-auto p-6 text-sm text-gray-500">
    © <span id="y"></span> Calvin Zheng
  </footer>
  <script>document.getElementById('y').textContent = new Date().getFullYear();</script>
</body>
</html>
