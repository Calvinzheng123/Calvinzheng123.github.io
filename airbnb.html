<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Project 3: Airbnb Price Regression</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <script src="https://cdn.tailwindcss.com"></script>
  <meta name="description" content="Regression modeling with Linear, Ridge, and Gradient Boosting on Airbnb Open Data — predicting nightly price, preprocessing, feature engineering, and evaluation."/>
</head>

<body class="bg-white text-gray-900">
  <header class="max-w-3xl mx-auto p-6">
    <a href="index.html" class="text-sm text-blue-600 underline">← Back</a>
    <h1 class="text-3xl font-bold mt-2">Project 3: Airbnb Price Regression</h1>
    <p class="mt-2 text-gray-700">
      Dataset:
      <a class="underline text-blue-600" href="https://www.kaggle.com/datasets/arianazmoudeh/airbnbopendata" target="_blank" rel="noopener">Airbnb Open Data (Kaggle)</a>
    </p>
    <p class="mt-1 text-gray-700">
      Goal: predict nightly Airbnb prices using numeric listing features. Compared Linear Regression, Ridge Regression, and Gradient Boosting models.
    </p>
  </header>

  <main class="max-w-3xl mx-auto p-6 space-y-10">
    <!-- ===== Story Section ===== -->
    <section>
      <h2 class="text-2xl font-semibold">Story: Modeling Airbnb Prices</h2>
      <p class="mt-2">
        Airbnb prices fluctuate widely based on location, season, and listing quality. Using the open dataset, I trained regression models to predict nightly price. 
        I started with simple linear regression, tested Ridge to address feature correlation, and finally tried Gradient Boosting to capture nonlinear effects.
      </p>

      <!-- Problem -->
      <div class="mt-6">
        <h3 class="text-xl font-semibold">Problem</h3>
        <p class="mt-1">
          The objective was to estimate listing price from numeric features such as location coordinates, reviews, and availability. 
          The challenge was limited signal since most important pricing factors (size, amenities, décor) aren’t captured in the dataset and it had a lot of useless features.
        </p>
      </div>

      <!-- Data -->
      <div class="mt-6">
        <h3 class="text-xl font-semibold">Data</h3>
        <p class="mt-1">
          The dataset contained ~100k listings and 26 columns. Many were categorical or text-heavy (e.g., host name, neighborhood, cancellation policy).  
          I focused on numeric variables: <code>lat</code>, <code>long</code>, <code>minimum nights</code>, <code>reviews per month</code>, 
          <code>number of reviews</code>, <code>availability 365</code>, <code>Construction year</code>, and several feature engineered ratios.
        </p>
      </div>

      <!-- Visuals -->
      <div class="mt-6">
        <h3 class="text-xl font-semibold">Feature Importance (Gradient Boosting)</h3>
        <figure class="mt-2">
          <img src="figs/airbnb_feature_importance.png" alt="Feature importance plot for GB model" class="w-full rounded border" />
          <figcaption class="text-sm text-gray-600 mt-1">
            Latitude, longitude, and review activity dominated price prediction. Policy and host features were nearly irrelevant.
          </figcaption>
        </figure>
      </div>

      <!-- Results -->
      <div class="mt-6">
        <h3 class="text-xl font-semibold">Results Summary</h3>
        <table class="w-full text-sm mt-3 border">
          <thead class="bg-gray-100 border-b">
            <tr>
              <th class="text-left p-2">Model</th>
              <th class="text-left p-2">RMSE ($)</th>
              <th class="text-left p-2">MAE ($)</th>
              <th class="text-left p-2">R²</th>
            </tr>
          </thead>
          <tbody>
            <tr class="border-b"><td class="p-2">Linear</td><td class="p-2">345.96</td><td class="p-2">294.51</td><td class="p-2">–0.000</td></tr>
            <tr class="border-b"><td class="p-2">Ridge</td><td class="p-2">345.96</td><td class="p-2">294.51</td><td class="p-2">–0.000</td></tr>
            <tr><td class="p-2">Gradient Boosting</td><td class="p-2">344.67</td><td class="p-2">293.16</td><td class="p-2">0.007</td></tr>
          </tbody>
        </table>
        <p class="mt-3">
          Gradient Boosting barely improved the fit, which confirmed for me that there was really limited predictive signal in available numeric fields.
        </p>
      </div>

      <!-- Learnings -->
      <div class="mt-6">
        <h3 class="text-xl font-semibold">Key Takeaways</h3>
        <ul class="list-disc pl-6 mt-2 space-y-1">
          <li>Data cleaning (price parsing, outlier trimming, log-transform) stabilized training and evaluation.</li>
          <li>Linear and Ridge models performed identically → no major collinearity issues.</li>
          <li>Gradient Boosting found tiny nonlinear gains, but overall R² ≈ 0 — the dataset lacks strong price predictors.</li>
          <li>Location and review intensity drive what little variance exists.</li>
        </ul>
      </div>

      <!-- Impact -->
      <div class="mt-6">
        <h3 class="text-xl font-semibold">Impact & Reflection</h3>
        <p class="mt-2">
          The project demonstrates realistic limits of modeling with incomplete data. 
          Automated pricing systems trained on shallow features risk biasing prices by geography, potentially reinforcing income or tourism inequality. 
          Responsible deployment would require richer, audited features including amenities, accessibility, and fairness checks.
        </p>
      </div>
    </section>

    <!-- ===== Technical Report ===== -->
    <section class="border-t pt-8">
      <h2 class="text-2xl font-semibold">Technical Report (Process & Reflection)</h2>

      <nav class="mt-4 text-sm text-blue-700">
        <ul class="list-disc pl-6 space-y-1">
          <li><a href="#dataset" class="underline">Dataset origin & description</a></li>
          <li><a href="#preproc" class="underline">Preprocessing & feature engineering</a></li>
          <li><a href="#exp" class="underline">Experiments 1–3</a></li>
          <li><a href="#impact" class="underline">Impact & limitations</a></li>
          <li><a href="#code" class="underline">Code & notebook</a></li>
        </ul>
      </nav>

      <!-- Dataset -->
      <section id="dataset" class="mt-6">
        <h3 class="text-xl font-semibold">Dataset origin & description</h3>
        <p class="mt-2">
          Source: <a class="underline text-blue-600" href="https://www.kaggle.com/datasets/arianazmoudeh/airbnbopendata" target="_blank" rel="noopener">Airbnb Open Data (Kaggle)</a>.  
          Contains public listing information (location, host, availability, review metrics, and policies). Target variable is nightly <code>price</code> in USD.
        </p>
      </section>

      <!-- Preproc -->
      <section id="preproc" class="mt-6">
        <h3 class="text-xl font-semibold">Preprocessing & Feature Engineering</h3>
        <ul class="list-disc pl-6 mt-2 space-y-2">
          <li>Removed categorical/text columns to focus on numeric data and avoid encoding complexity.</li>
          <li>Cleaned monetary strings into floats; removed zero/negative prices.</li>
          <li>Trimmed 1st–99th percentile to limit outliers.</li>
          <li>Log-transformed target (<code>log_price = log1p(price)</code>) for stable residuals.</li>
          <li>Engineered simple features:
            <ul class="list-disc pl-6 mt-1">
              <li><code>review_ratio = reviews per month / (listings count + 1)</code></li>
              <li><code>age_factor = 2025 – Construction year</code></li>
            </ul>
          </li>
          <li>Dropped rows with missing numeric values after cleaning.</li>
        </ul>
      </section>

      <section id="exp" class="mt-6">
        <h3 class="text-xl font-semibold">Experiments 1–3</h3>
        <ul class="list-disc pl-6 mt-2 space-y-4">
          <li>
            <strong>Exp 1 — Linear Regression (Baseline):</strong>
            <p class="mt-1">
              The goal of this first experiment was to establish a simple baseline using standard <code>LinearRegression</code>. 
              Linear models are easy to interpret and highlight whether a straightforward, additive relationship exists between predictors 
              and price. This model assumes that features like latitude, longitude, reviews, and availability have a direct linear influence on 
              log-transformed price. 
            </p>
            <p class="mt-1">
              The result — <strong>RMSE ≈ 345.96, MAE ≈ 294.51, R² ≈ 0.0</strong> — revealed almost no linear correlation between our selected features 
              and the target variable. This suggested that the relationship between Airbnb prices and available metadata is either weak or highly nonlinear.
            </p>
          </li>
      
          <li>
            <strong>Exp 2 — Ridge Regression (Regularized Linear):</strong>
            <p class="mt-1">
              Next, I tested Ridge Regression with <strong>L2 regularization (α = 1.0)</strong> to investigate whether multicollinearity among 
              numeric variables (like review counts and availability) was inflating noise in the linear model. Ridge penalizes large coefficient values, 
              which can stabilize estimates and slightly reduce overfitting when features are correlated.
            </p>
            <p class="mt-1">
              The outcome — <strong>RMSE ≈ 345.96, MAE ≈ 294.51, R² ≈ 0.0</strong> — was virtually identical to the baseline. This confirmed that 
              the model’s lack of explanatory power wasn’t due to collinearity but rather insufficient predictive signal within the features themselves.
            </p>
          </li>
      
          <li>
            <strong>Exp 3 — Gradient Boosting (Nonlinear Tree Ensemble):</strong>
            <p class="mt-1">
              Finally, I applied a <strong>Gradient Boosting Regressor</strong> 
              (<code>n_estimators=400</code>, <code>learning_rate=0.05</code>, <code>max_depth=3</code>) to capture potential nonlinear 
              patterns — such as interactions between geographic coordinates and review behavior — that linear models cannot represent.
            </p>
            <p class="mt-1">
              The result — <strong>RMSE ≈ 344.67, MAE ≈ 293.16, R² ≈ 0.007</strong> — indicated a minor improvement, explaining less than 
              1% of the variance. This small gain implies that while weak nonlinear structure exists (mostly tied to latitude and longitude), 
              the dataset lacks richer descriptive or contextual features to make substantial price predictions.
            </p>
            <p class="mt-1">
              Overall, the three experiments demonstrate that model complexity alone cannot compensate for missing information. 
              Improvements will depend on introducing new features (e.g., property type, amenities, or proximity to attractions) 
              rather than tuning algorithms.
            </p>
          </li>
        </ul>
      </section>

      <!-- Impact -->
      <section id="impact" class="mt-6">
        <h3 class="text-xl font-semibold">Impact & Limitations</h3>
        <p class="mt-2">
          Price predictions based only on limited numeric metadata are unreliable. Real Airbnb pricing depends on amenities, 
          property size, seasonality, and demand context — none captured here. 
          Still, the workflow demonstrates sound regression design and the importance of feature richness over model complexity.
        </p>
      </section>

      <!-- Code -->
      <section id="code" class="mt-6">
        <h3 class="text-xl font-semibold">Code & Notebook</h3>
        <div class="space-y-2 mt-2">
          <p><a class="underline text-blue-600" href="notebooks/AirbnbRegression.ipynb" download>Download Notebook</a></p>
          <div class="space-x-3 mt-2">
            <a class="inline-block px-3 py-2 rounded bg-black text-white" href="https://colab.research.google.com/github/Calvinzheng123/Calvinzheng123.github.io/blob/main/notebooks/AirbnbRegression.ipynb" target="_blank" rel="noopener">Open in Colab</a>
            <a class="inline-block px-3 py-2 rounded bg-gray-200" href="https://nbviewer.org/github/Calvinzheng123/Calvinzheng123.github.io/blob/main/notebooks/AirbnbRegression.ipynb" target="_blank" rel="noopener">View on nbviewer</a>
          </div>
        </div>
      </section>

      <!-- References -->
      <section class="mt-8">
        <h3 class="text-xl font-semibold">References</h3>
        <ul class="list-disc pl-6 mt-2">
          <li>Dataset: <a class="underline text-blue-600" href="https://www.kaggle.com/datasets/arianazmoudeh/airbnbopendata" target="_blank" rel="noopener">Airbnb Open Data (Kaggle)</a></li>
          <li>Libraries: pandas, scikit-learn</li>
          <li>Transparency: ChatGPT assisted with structuring text and code formatting for clarity.</li>
        </ul>
      </section>
    </section>
  </main>

  <footer class="max-w-3xl mx-auto p-6 text-sm text-gray-500">
    © <span id="y"></span> Calvin Zheng
  </footer>
  <script>document.getElementById('y').textContent = new Date().getFullYear();</script>
</body>
</html>
